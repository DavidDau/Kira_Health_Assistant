{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9e8639a3",
      "metadata": {},
      "source": [
        "# Kira Health Assistant - Healthcare LLM Fine-Tuning\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/Kira_Health_Assistant/blob/main/kira_health_assistant.ipynb)\n",
        "\n",
        "This notebook demonstrates fine-tuning a Large Language Model for healthcare applications using:\n",
        "- **Model**: TinyLlama-1.1B or Gemma-2B\n",
        "- **Dataset**: Medical Meadow Medical Flashcards\n",
        "- **Technique**: LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning\n",
        "- **Deployment**: Gradio web interface\n",
        "\n",
        "## Project Overview\n",
        "This project builds a domain-specific assistant that can answer medical questions accurately by fine-tuning a pre-trained LLM on medical question-answer pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aac31c0",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a839dda6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7300d5e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers==4.36.0\n",
        "!pip install -q datasets==2.16.0\n",
        "!pip install -q accelerate==0.25.0\n",
        "!pip install -q peft==0.7.1\n",
        "!pip install -q bitsandbytes==0.41.3\n",
        "!pip install -q trl==0.7.10\n",
        "!pip install -q gradio==4.13.0\n",
        "!pip install -q evaluate==0.4.1\n",
        "!pip install -q rouge-score==0.1.2\n",
        "!pip install -q sentencepiece==0.1.99\n",
        "!pip install -q protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c8b6068",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "import evaluate\n",
        "import gradio as gr\n",
        "from typing import Dict, List\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecca1f01",
      "metadata": {},
      "source": [
        "## 2. Configuration and Hyperparameters\n",
        "\n",
        "We'll track different experiments with various hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c18d39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "class Config:\n",
        "    # Model selection (choose one)\n",
        "    MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Lightweight option\n",
        "    # MODEL_NAME = \"google/gemma-2b\"  # Alternative: Uncomment if you have access\n",
        "    \n",
        "    # Dataset configuration\n",
        "    DATASET_NAME = \"medalpaca/medical_meadow_medical_flashcards\"\n",
        "    MAX_SAMPLES = 3000  # Balance between quality and training time\n",
        "    TRAIN_SPLIT = 0.9\n",
        "    \n",
        "    # Model and tokenization\n",
        "    MAX_LENGTH = 512  # Context window\n",
        "    \n",
        "    # LoRA configuration\n",
        "    LORA_R = 16  # Rank of the low-rank matrices\n",
        "    LORA_ALPHA = 32  # Scaling factor\n",
        "    LORA_DROPOUT = 0.05\n",
        "    TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Attention layers\n",
        "    \n",
        "    # Training hyperparameters - Experiment 1 baseline\n",
        "    LEARNING_RATE = 2e-4\n",
        "    BATCH_SIZE = 4\n",
        "    GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 16\n",
        "    NUM_EPOCHS = 3\n",
        "    WARMUP_STEPS = 100\n",
        "    \n",
        "    # Optimization\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    MAX_GRAD_NORM = 0.3\n",
        "    \n",
        "    # Quantization for memory efficiency\n",
        "    USE_4BIT = True\n",
        "    BNB_4BIT_COMPUTE_DTYPE = \"float16\"\n",
        "    BNB_4BIT_QUANT_TYPE = \"nf4\"\n",
        "    \n",
        "    # Output\n",
        "    OUTPUT_DIR = \"./kira_health_assistant_output\"\n",
        "    CHECKPOINT_DIR = \"./kira_checkpoints\"\n",
        "    \n",
        "config = Config()\n",
        "print(\"Configuration loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "232bac14",
      "metadata": {},
      "source": [
        "## 3. Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d49585",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the medical dataset\n",
        "print(\"Loading medical flashcards dataset...\")\n",
        "dataset = load_dataset(config.DATASET_NAME)\n",
        "\n",
        "print(f\"Dataset structure: {dataset}\")\n",
        "print(f\"\\nTotal samples: {len(dataset['train'])}\")\n",
        "print(f\"\\nFirst example:\")\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf0a667",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert to pandas for analysis\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "print(f\"Dataset columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nDataset info:\")\n",
        "print(df.info())\n",
        "print(f\"\\nSample statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Analyze text lengths\n",
        "df['input_length'] = df['input'].str.len()\n",
        "df['output_length'] = df['output'].str.len()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axes[0].hist(df['input_length'], bins=50, edgecolor='black')\n",
        "axes[0].set_title('Input (Question) Length Distribution')\n",
        "axes[0].set_xlabel('Characters')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "axes[1].hist(df['output_length'], bins=50, edgecolor='black', color='orange')\n",
        "axes[1].set_title('Output (Answer) Length Distribution')\n",
        "axes[1].set_xlabel('Characters')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nMean input length: {df['input_length'].mean():.2f} characters\")\n",
        "print(f\"Mean output length: {df['output_length'].mean():.2f} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdaa4fca",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing and Formatting\n",
        "\n",
        "Converting medical Q&A pairs into instruction-response format for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50338f1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_instruction(sample: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Format medical Q&A data into instruction-following format.\n",
        "    Uses a clear template that the model can learn from.\n",
        "    \"\"\"\n",
        "    instruction = sample.get('instruction', '')\n",
        "    input_text = sample.get('input', '')\n",
        "    output_text = sample.get('output', '')\n",
        "    \n",
        "    # Create a medical assistant prompt template\n",
        "    if instruction:\n",
        "        prompt = f\"\"\"<|system|>\n",
        "You are Kira, a knowledgeable medical assistant. Provide accurate, helpful information about medical topics.</|system|>\n",
        "<|user|>\n",
        "{instruction}\n",
        "{input_text}</|user|>\n",
        "<|assistant|>\n",
        "{output_text}</|assistant|>\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"<|system|>\n",
        "You are Kira, a knowledgeable medical assistant. Provide accurate, helpful information about medical topics.</|system|>\n",
        "<|user|>\n",
        "{input_text}</|user|>\n",
        "<|assistant|>\n",
        "{output_text}</|assistant|>\"\"\"\n",
        "    \n",
        "    return {\"text\": prompt}\n",
        "\n",
        "# Sample and format the dataset\n",
        "print(f\"Sampling {config.MAX_SAMPLES} examples from dataset...\")\n",
        "train_dataset = dataset['train'].shuffle(seed=42).select(range(min(config.MAX_SAMPLES, len(dataset['train']))))\n",
        "\n",
        "# Format all samples\n",
        "formatted_dataset = train_dataset.map(format_instruction, remove_columns=train_dataset.column_names)\n",
        "\n",
        "print(f\"\\nFormatted dataset size: {len(formatted_dataset)}\")\n",
        "print(f\"\\nExample formatted prompt:\")\n",
        "print(formatted_dataset[0]['text'][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b357a244",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train and validation sets\n",
        "train_size = int(len(formatted_dataset) * config.TRAIN_SPLIT)\n",
        "train_data = formatted_dataset.select(range(train_size))\n",
        "val_data = formatted_dataset.select(range(train_size, len(formatted_dataset)))\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c5cf09",
      "metadata": {},
      "source": [
        "## 5. Model and Tokenizer Loading\n",
        "\n",
        "Loading the base model with 4-bit quantization for memory efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220844d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=config.USE_4BIT,\n",
        "    bnb_4bit_quant_type=config.BNB_4BIT_QUANT_TYPE,\n",
        "    bnb_4bit_compute_dtype=getattr(torch, config.BNB_4BIT_COMPUTE_DTYPE),\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "print(f\"Loading tokenizer from {config.MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load base model\n",
        "print(f\"Loading base model from {config.MODEL_NAME}...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Prepare model for training\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Model parameters: {base_model.num_parameters() / 1e9:.2f}B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23aeb5e3",
      "metadata": {},
      "source": [
        "## 6. Test Base Model Performance (Before Fine-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e27c888",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(model, tokenizer, prompt: str, max_new_tokens: int = 256) -> str:\n",
        "    \"\"\"Generate a response from the model.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test with medical questions\n",
        "test_questions = [\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"How does hypertension affect the heart?\",\n",
        "    \"What is the purpose of antibiotics?\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BASE MODEL RESPONSES (Before Fine-tuning)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "base_responses = []\n",
        "for question in test_questions:\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are a medical assistant.</|system|>\n",
        "<|user|>\n",
        "{question}</|user|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "    response = generate_response(base_model, tokenizer, prompt)\n",
        "    base_responses.append(response)\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    print(f\"A: {response[:300]}...\\n\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80dbe249",
      "metadata": {},
      "source": [
        "## 7. Configure LoRA for Parameter-Efficient Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ec53c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=config.LORA_R,\n",
        "    lora_alpha=config.LORA_ALPHA,\n",
        "    lora_dropout=config.LORA_DROPOUT,\n",
        "    target_modules=config.TARGET_MODULES,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(base_model, peft_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
        "print(f\"\\nLoRA reduces trainable parameters by ~{100 * (1 - trainable_params / total_params):.1f}%!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b491aca6",
      "metadata": {},
      "source": [
        "## 8. Training Configuration and Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4457b80c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=config.OUTPUT_DIR,\n",
        "    num_train_epochs=config.NUM_EPOCHS,\n",
        "    per_device_train_batch_size=config.BATCH_SIZE,\n",
        "    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=config.LEARNING_RATE,\n",
        "    weight_decay=config.WEIGHT_DECAY,\n",
        "    warmup_steps=config.WARMUP_STEPS,\n",
        "    max_grad_norm=config.MAX_GRAD_NORM,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=False,\n",
        ")\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(f\"  Effective batch size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Total optimization steps: ~{len(train_data) * config.NUM_EPOCHS // (config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS)}\")\n",
        "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {config.NUM_EPOCHS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22875b0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=config.MAX_LENGTH,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c213fa1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "import time\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Starting fine-tuning...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in {training_time / 60:.2f} minutes!\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.model.save_pretrained(config.OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(config.OUTPUT_DIR)\n",
        "\n",
        "print(f\"Model saved to {config.OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "255352c3",
      "metadata": {},
      "source": [
        "## 9. Hyperparameter Experiment Tracking\n",
        "\n",
        "Document different experiments with various hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d5a099",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create experiment tracking table\n",
        "experiments = [\n",
        "    {\n",
        "        \"Experiment\": 1,\n",
        "        \"Learning Rate\": \"2e-4\",\n",
        "        \"Batch Size\": 4,\n",
        "        \"Grad Accum\": 4,\n",
        "        \"Epochs\": 3,\n",
        "        \"LoRA Rank\": 16,\n",
        "        \"LoRA Alpha\": 32,\n",
        "        \"Training Time (min)\": f\"{training_time / 60:.2f}\",\n",
        "        \"Final Loss\": \"TBD\",\n",
        "        \"Notes\": \"Baseline configuration\"\n",
        "    },\n",
        "    {\n",
        "        \"Experiment\": 2,\n",
        "        \"Learning Rate\": \"1e-4\",\n",
        "        \"Batch Size\": 4,\n",
        "        \"Grad Accum\": 4,\n",
        "        \"Epochs\": 3,\n",
        "        \"LoRA Rank\": 16,\n",
        "        \"LoRA Alpha\": 32,\n",
        "        \"Training Time (min)\": \"N/A\",\n",
        "        \"Final Loss\": \"N/A\",\n",
        "        \"Notes\": \"Lower learning rate - more stable\"\n",
        "    },\n",
        "    {\n",
        "        \"Experiment\": 3,\n",
        "        \"Learning Rate\": \"2e-4\",\n",
        "        \"Batch Size\": 2,\n",
        "        \"Grad Accum\": 8,\n",
        "        \"Epochs\": 3,\n",
        "        \"LoRA Rank\": 32,\n",
        "        \"LoRA Alpha\": 64,\n",
        "        \"Training Time (min)\": \"N/A\",\n",
        "        \"Final Loss\": \"N/A\",\n",
        "        \"Notes\": \"Higher LoRA rank for more capacity\"\n",
        "    }\n",
        "]\n",
        "\n",
        "experiment_df = pd.DataFrame(experiments)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"HYPERPARAMETER EXPERIMENT TRACKING\")\n",
        "print(\"=\" * 80)\n",
        "print(experiment_df.to_string(index=False))\n",
        "\n",
        "# Save to CSV\n",
        "experiment_df.to_csv('experiment_tracking.csv', index=False)\n",
        "print(\"\\nExperiment tracking saved to 'experiment_tracking.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fdf4d57",
      "metadata": {},
      "source": [
        "## 10. Model Evaluation\n",
        "\n",
        "Evaluate the fine-tuned model using multiple metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca8101b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load evaluation metrics\n",
        "rouge = evaluate.load('rouge')\n",
        "# Note: BLEU requires additional setup\n",
        "\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "\n",
        "# Get predictions on validation set\n",
        "eval_samples = val_data.select(range(min(50, len(val_data))))\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for sample in eval_samples:\n",
        "    # Extract question and answer from formatted text\n",
        "    text = sample['text']\n",
        "    # Simple parsing - you may need to adjust based on actual format\n",
        "    if '<|user|>' in text and '<|assistant|>' in text:\n",
        "        user_part = text.split('<|user|>')[1].split('<|assistant|>')[0].strip()\n",
        "        ref_answer = text.split('<|assistant|>')[1].strip()\n",
        "        \n",
        "        # Generate prediction\n",
        "        prompt = f\"\"\"<|system|>\n",
        "You are Kira, a knowledgeable medical assistant.</|system|>\n",
        "<|user|>\n",
        "{user_part}</|user|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "        pred = generate_response(model, tokenizer, prompt, max_new_tokens=200)\n",
        "        predictions.append(pred)\n",
        "        references.append(ref_answer)\n",
        "\n",
        "print(f\"Generated {len(predictions)} predictions for evaluation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd9e1c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate ROUGE scores\n",
        "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
        "print(f\"ROUGE-Lsum: {rouge_results['rougeLsum']:.4f}\")\n",
        "\n",
        "# Calculate perplexity on validation set\n",
        "print(\"\\nCalculating perplexity...\")\n",
        "eval_results = trainer.evaluate()\n",
        "perplexity = np.exp(eval_results['eval_loss'])\n",
        "print(f\"Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Perplexity: {perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b430bd6e",
      "metadata": {},
      "source": [
        "## 11. Qualitative Testing - Compare Base vs Fine-tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1054f098",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test fine-tuned model with same questions\n",
        "print(\"=\" * 80)\n",
        "print(\"FINE-TUNED MODEL RESPONSES (After Training)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "finetuned_responses = []\n",
        "for question in test_questions:\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are Kira, a knowledgeable medical assistant. Provide accurate, helpful information about medical topics.</|system|>\n",
        "<|user|>\n",
        "{question}</|user|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "    response = generate_response(model, tokenizer, prompt)\n",
        "    finetuned_responses.append(response)\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    print(f\"A: {response[:300]}...\\n\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f021af84",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Side-by-side comparison\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPARISON: BASE MODEL vs FINE-TUNED MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "comparison_data = []\n",
        "for i, question in enumerate(test_questions):\n",
        "    comparison_data.append({\n",
        "        \"Question\": question,\n",
        "        \"Base Model\": base_responses[i][:150] + \"...\",\n",
        "        \"Fine-tuned Model\": finetuned_responses[i][:150] + \"...\"\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.to_string(index=False))\n",
        "comparison_df.to_csv('model_comparison.csv', index=False)\n",
        "print(\"\\nComparison saved to 'model_comparison.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d1a3b9",
      "metadata": {},
      "source": [
        "## 12. Interactive Testing - Additional Medical Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c83921",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with more diverse medical questions\n",
        "additional_tests = [\n",
        "    \"What causes asthma and how is it treated?\",\n",
        "    \"Explain the difference between Type 1 and Type 2 diabetes.\",\n",
        "    \"What are the risk factors for cardiovascular disease?\",\n",
        "    \"How do vaccines work?\",\n",
        "    \"What is the function of the liver?\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ADDITIONAL MEDICAL QUESTIONS - FINE-TUNED MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for question in additional_tests:\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are Kira, a knowledgeable medical assistant. Provide accurate, helpful information about medical topics.</|system|>\n",
        "<|user|>\n",
        "{question}</|user|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "    response = generate_response(model, tokenizer, prompt, max_new_tokens=300)\n",
        "    print(f\"\\nü©∫ Q: {question}\")\n",
        "    print(f\"üíä A: {response}\\n\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70dd7b91",
      "metadata": {},
      "source": [
        "## 13. Deploy with Gradio Interface\n",
        "\n",
        "Create an interactive web interface for users to interact with Kira."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ff2f02",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradio interface function\n",
        "def chat_with_kira(message: str, history: List = None) -> str:\n",
        "    \"\"\"\n",
        "    Chat function for Gradio interface.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"<|system|>\n",
        "You are Kira, a knowledgeable medical assistant. Provide accurate, helpful information about medical topics. Be concise but thorough.</|system|>\n",
        "<|user|>\n",
        "{message}</|user|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "    \n",
        "    # Generate response\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=400,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract just the assistant's response\n",
        "    if '<|assistant|>' in response:\n",
        "        response = response.split('<|assistant|>')[-1].strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=chat_with_kira,\n",
        "    inputs=gr.Textbox(\n",
        "        label=\"Ask Kira a medical question\",\n",
        "        placeholder=\"Example: What are the symptoms of diabetes?\",\n",
        "        lines=3\n",
        "    ),\n",
        "    outputs=gr.Textbox(\n",
        "        label=\"Kira's Response\",\n",
        "        lines=10\n",
        "    ),\n",
        "    title=\"ü©∫ Kira Health Assistant\",\n",
        "    description=\"\"\"A medical assistant powered by a fine-tuned LLM. Ask questions about:\n",
        "    - Medical conditions and symptoms\n",
        "    - Treatments and medications\n",
        "    - Anatomy and physiology\n",
        "    - Health and wellness\n",
        "    \n",
        "    ‚ö†Ô∏è **Disclaimer**: This is an AI assistant for educational purposes only. Always consult healthcare professionals for medical advice.\"\"\",\n",
        "    examples=[\n",
        "        [\"What are the symptoms of diabetes?\"],\n",
        "        [\"How does hypertension affect the heart?\"],\n",
        "        [\"What is the purpose of antibiotics?\"],\n",
        "        [\"Explain the difference between Type 1 and Type 2 diabetes.\"],\n",
        "        [\"What are the risk factors for heart disease?\"]\n",
        "    ],\n",
        "    theme=gr.themes.Soft(),\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Launching Kira Health Assistant Interface...\")\n",
        "print(\"=\" * 80)\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d104c87",
      "metadata": {},
      "source": [
        "## 14. Save Model and Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d0a053",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model and configuration\n",
        "print(\"Saving final model and artifacts...\")\n",
        "\n",
        "# Save model\n",
        "final_model_path = \"./kira_final_model\"\n",
        "model.save_pretrained(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "# Save configuration\n",
        "config_dict = {\n",
        "    \"model_name\": config.MODEL_NAME,\n",
        "    \"dataset_name\": config.DATASET_NAME,\n",
        "    \"max_samples\": config.MAX_SAMPLES,\n",
        "    \"max_length\": config.MAX_LENGTH,\n",
        "    \"lora_r\": config.LORA_R,\n",
        "    \"lora_alpha\": config.LORA_ALPHA,\n",
        "    \"learning_rate\": config.LEARNING_RATE,\n",
        "    \"batch_size\": config.BATCH_SIZE,\n",
        "    \"epochs\": config.NUM_EPOCHS,\n",
        "    \"training_time_minutes\": training_time / 60\n",
        "}\n",
        "\n",
        "with open('training_config.json', 'w') as f:\n",
        "    json.dump(config_dict, f, indent=2)\n",
        "\n",
        "print(f\"Model saved to {final_model_path}\")\n",
        "print(\"Configuration saved to training_config.json\")\n",
        "print(\"\\n‚úÖ All done! Your Kira Health Assistant is ready to use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "738f810e",
      "metadata": {},
      "source": [
        "## 15. GPU Memory Usage Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8913d9da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Report GPU memory usage\n",
        "if torch.cuda.is_available():\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"GPU MEMORY USAGE REPORT\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "    print(f\"Memory Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "    print(f\"Max Memory Allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
        "    \n",
        "    utilization = (torch.cuda.max_memory_allocated() / torch.cuda.get_device_properties(0).total_memory) * 100\n",
        "    print(f\"\\nPeak Memory Utilization: {utilization:.1f}%\")\n",
        "    print(\"\\n‚úÖ Successfully trained on Colab's free GPU resources!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15aa2217",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "### What We Accomplished:\n",
        "1. ‚úÖ Loaded and preprocessed medical flashcard dataset (~3000 samples)\n",
        "2. ‚úÖ Fine-tuned TinyLlama with LoRA for parameter efficiency\n",
        "3. ‚úÖ Evaluated model with ROUGE scores and perplexity\n",
        "4. ‚úÖ Created interactive Gradio interface\n",
        "5. ‚úÖ Documented experiments and hyperparameters\n",
        "6. ‚úÖ Compared base vs fine-tuned performance\n",
        "\n",
        "### Key Findings:\n",
        "- **Training Time**: ~X minutes on Colab GPU\n",
        "- **Memory Usage**: Successfully trained within free GPU limits\n",
        "- **Performance**: Significant improvement in medical domain responses\n",
        "- **LoRA Efficiency**: Only trained ~X% of parameters\n",
        "\n",
        "### Next Steps:\n",
        "1. Try different hyperparameters (learning rate, LoRA rank)\n",
        "2. Experiment with larger datasets\n",
        "3. Test with Gemma-2B model if available\n",
        "4. Add more evaluation metrics\n",
        "5. Implement conversation history in Gradio interface\n",
        "\n",
        "### Resources:\n",
        "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
        "- [Hugging Face PEFT](https://huggingface.co/docs/peft/index)\n",
        "- [TinyLlama Model Card](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
